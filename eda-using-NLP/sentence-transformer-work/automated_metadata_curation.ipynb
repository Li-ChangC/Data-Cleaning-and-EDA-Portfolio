{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "CR0tdzlc5zhQ",
      "metadata": {
        "id": "CR0tdzlc5zhQ"
      },
      "source": [
        "\n",
        "## The problem\n",
        "\n",
        "We have a set of terms (sometimes multi-word) that represent a meaning. We want to map those terms to other terms from an ontology or controlled vocabulary, but we want to do so using their meaning, not just text matching. \"Encodings\" are a way of transforming words or sentences into vectors of numbers such that the points in N-dimensional space that are near each other have similar meanings. We use that idea here to map\n",
        "\n",
        "uncurated term ---> curated term\n",
        "\n",
        "We do this by:\n",
        "\n",
        "curated term ---> encodings\n",
        "\n",
        "uncurated term ---> encodings\n",
        "\n",
        "\n",
        "## Sentence transformers\n",
        "\n",
        "Sentence transformers refer to a type of natural language processing (NLP) model designed specifically for transforming sentences or text snippets into fixed-dimensional vectors, often with the goal of capturing semantic similarity. These models use deep learning techniques, typically employing architectures like Siamese networks or Transformer models.\n",
        "\n",
        "The primary objective of sentence transformers is to generate embeddings or representations of sentences in a way that the distance or similarity between these embeddings reflects the semantic meaning of the corresponding sentences. This makes them useful for various NLP tasks such as sentence similarity, clustering, and information retrieval.\n",
        "\n",
        "Commonly used architectures for sentence transformers include BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly optimized BERT approach), and DistilBERT, among others. Pre-trained transformer models can be fine-tuned on specific tasks or datasets to create sentence embeddings tailored to a particular application.\n",
        "\n",
        "Sentence embeddings obtained from these models can be useful in a variety of applications, including semantic search, document retrieval, and sentiment analysis, where understanding the underlying semantic relationships between sentences is crucial.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Phlsz8WBPdKd",
      "metadata": {
        "id": "Phlsz8WBPdKd"
      },
      "source": [
        "### Value-based curation\n",
        "#### 1. Choose and import sentence transformer for our job\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "N2Mb-LDaX58H",
      "metadata": {
        "id": "N2Mb-LDaX58H"
      },
      "outputs": [],
      "source": [
        "# %pip install sentence-transformers pandas numpy scikit-learn\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine, cdist\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer(\"kamalkraj/BioSimCSE-BioLinkBert-Base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88bgbE6A7Dpw",
      "metadata": {
        "id": "88bgbE6A7Dpw"
      },
      "source": [
        "#### 2. Prepare data for model training\n",
        "Load the `curated_bodysite.csv` file and create a set of \"original\" (uncurated) values and a set of \"curated\" values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i_fNfSQ87auw",
      "metadata": {
        "id": "i_fNfSQ87auw"
      },
      "source": [
        "Our task is to use the \"curated\" set, which corresponds to our controlled vocabulary or ontology terms, as a dictionary of sorts. We want to provide an index to that dictionary that allows us to look up words by **their meaning**. The emdeddings are the \"meaning\" and we can then use those as our index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4db2d6a2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SALIVARY GLAND -> NCIT:C12426\n",
            "LUNG -> NCIT:C12468\n",
            "BUCCAL MUCOSA -> NCIT:C12505\n",
            "BONE -> NCIT:C12366\n",
            "ORBIT -> NCIT:C12347\n"
          ]
        }
      ],
      "source": [
        "curated_df = pd.read_csv(\"curated_bodysite.csv\")\n",
        "\n",
        "# On a bigger view of the task, we can see that we will use curated_bodysite_ontology_term_id along with curated_bodysite, so it's better to map them for better efficiency\n",
        "\n",
        "# Drop null and standardize the original_bodysite column and the curated_bodysite column\n",
        "df = curated_df.dropna(subset=[\"curated_bodysite\", \"curated_bodysite_ontology_term_id\"]).copy()\n",
        "df[\"curated_bodysite_clean\"] = df[\"curated_bodysite\"].str.strip().str.upper()\n",
        "df[\"original_bodysite_clean\"] = df[\"original_bodysite\"].str.strip().str.upper()\n",
        "df = df[~df[\"original_bodysite_clean\"].str.contains(\"LOCATION\")]\n",
        "\n",
        "# If we take a deep look on the dataset, we can see that \"curated_bodysite\" may contain multiple bodysites.\n",
        "df[\"curated_bodysite_split\"] = df[\"curated_bodysite_clean\"].str.split(\"<;>\")\n",
        "df[\"ontology_term_id_split\"] = df[\"curated_bodysite_ontology_term_id\"].str.split(\"<;>\")\n",
        "df[\"original_bodysite_split\"] = df[\"original_bodysite_clean\"].str.split(\"<;>\")\n",
        "\n",
        "# Check if each row have same number of splits\n",
        "df[\"bodysite_count\"] = df[\"curated_bodysite_split\"].apply(len)\n",
        "df[\"ontology_id_count\"] = df[\"ontology_term_id_split\"].apply(len)\n",
        "df[\"original_bodysite_count\"] = df[\"original_bodysite_split\"].apply(len)\n",
        "\n",
        "# Drop rows with mismatched splits. We only need examples where the number of bodysites and ontology term ids match\n",
        "df = df[(df[\"bodysite_count\"] == df[\"ontology_id_count\"]) & (df[\"bodysite_count\"] == df[\"original_bodysite_count\"])]\n",
        "\n",
        "# Map the bodysite to the ontology term id\n",
        "exploded_df = df.explode([\"curated_bodysite_split\", \"ontology_term_id_split\"])\n",
        "curated_to_id_map = dict(zip(exploded_df[\"curated_bodysite_split\"], exploded_df[\"ontology_term_id_split\"]))\n",
        "\n",
        "for term, ontology_id in list(curated_to_id_map.items())[:5]:\n",
        "    print(f\"{term} -> {ontology_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TVhkTQunRwuw",
      "metadata": {
        "id": "TVhkTQunRwuw"
      },
      "source": [
        "#### 3. Embed the curated values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "nEEGSM7SX7zp",
      "metadata": {
        "id": "nEEGSM7SX7zp"
      },
      "outputs": [],
      "source": [
        "curated_terms = list(curated_to_id_map.keys())\n",
        "curated_embeddings = model.encode(curated_terms)\n",
        "\n",
        "exploded_original_df = df.explode(\"original_bodysite_split\")\n",
        "uncurated_terms = exploded_original_df[\"original_bodysite_split\"].unique()\n",
        "uncurated_embeddings = model.encode(list(uncurated_terms))\n",
        "\n",
        "# Compute cosine distances between each uncurated embedding and all curated embeddings\n",
        "distance_matrix = cdist(uncurated_embeddings, curated_embeddings, metric=\"cosine\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aVUXquKHR3Y-",
      "metadata": {
        "id": "aVUXquKHR3Y-"
      },
      "source": [
        "#### 4. Map uncurated terms to curated terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2d3747a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final result:\n",
            "                     original_term mapped_curated_term1 mapped_curated_term2  \\\n",
            "0              \"BRAIN, CEREBELLUM\"                BRAIN           CEREBELLUM   \n",
            "1                \"BRAIN, PARIETAL\"        PARIETAL LOBE             PARIETAL   \n",
            "2                    3RD VENTRICLE      THIRD VENTRICLE     FOURTH VENTRICLE   \n",
            "3                    4TH VENTRICLE     FOURTH VENTRICLE      THIRD VENTRICLE   \n",
            "4                          ABDOMEN              ABDOMEN               PELVIS   \n",
            "..                             ...                  ...                  ...   \n",
            "390                         UTERUS               UTERUS         CERVIX UTERI   \n",
            "391                         VAGINA               VAGINA              URETHRA   \n",
            "392                     VENTRICLES         CERVIX UTERI              ABDOMEN   \n",
            "393  VERY DISTAL RECTUM RECURRENCE               RECTUM               DISTAL   \n",
            "394                          VULVA                VULVA       GASTRIC CARDIA   \n",
            "\n",
            "          mapped_curated_term3 mapped_curated_term4 mapped_curated_term5  \\\n",
            "0              BRAIN VENTRICLE           BRAIN STEM             PANCREAS   \n",
            "1                        BRAIN           BRAIN STEM    PARIETO-OCCIPITAL   \n",
            "2              BRAIN VENTRICLE          T3 VERTEBRA          L2 VERTEBRA   \n",
            "3              BRAIN VENTRICLE          L2 VERTEBRA          C4 VERTEBRA   \n",
            "4                       THORAX          MEDIASTINUM                CHEST   \n",
            "..                         ...                  ...                  ...   \n",
            "390                     VAGINA               RECTUM            ESOPHAGUS   \n",
            "391                    TRACHEA               UTERUS            ESOPHAGUS   \n",
            "392                     PALATE          MEDIASTINUM              URETHRA   \n",
            "393  SMALL INTESTINE RESECTION       TERMINAL ILEUM   DISTANT METASTASIS   \n",
            "394              OMENTAL BURSA               VAGINA             THALAMUS   \n",
            "\n",
            "    best_ontology_term_id  best_cosine_distance  \n",
            "0             NCIT:C12439          5.106297e-01  \n",
            "1             NCIT:C12354          4.694099e-01  \n",
            "2             NCIT:C12827          2.258774e-01  \n",
            "3             NCIT:C12828          1.912543e-01  \n",
            "4             NCIT:C12664          0.000000e+00  \n",
            "..                    ...                   ...  \n",
            "390           NCIT:C12405          0.000000e+00  \n",
            "391           NCIT:C12407          0.000000e+00  \n",
            "392           NCIT:C12311          4.034148e-01  \n",
            "393           NCIT:C12390          5.229178e-01  \n",
            "394           NCIT:C12408          1.110223e-16  \n",
            "\n",
            "[395 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "for i, uncurated_term in enumerate(uncurated_terms):\n",
        "    sorted_indices = np.argsort(distance_matrix[i])\n",
        "    for idx in sorted_indices[:5]:\n",
        "        curated_term = curated_terms[idx]\n",
        "        ontology_id = curated_to_id_map[curated_term]\n",
        "        distance = distance_matrix[i][idx]\n",
        "        results.append({\n",
        "            \"original_term\": uncurated_term,\n",
        "            \"mapped_curated_term\": curated_term,\n",
        "            \"ontology_term_id\": ontology_id,\n",
        "            \"cosine_distance\": distance\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Group by original term and select the top 5 matches based on cosine distance\n",
        "top5_per_term = results_df.sort_values(\"cosine_distance\")\\\n",
        "    .groupby(\"original_term\", as_index=False)\\\n",
        "    .head(5)\n",
        "\n",
        "top5_per_term[\"rank\"] = top5_per_term.groupby(\"original_term\").cumcount() + 1\n",
        "pivot_df = top5_per_term.pivot(index=\"original_term\", columns=\"rank\", values=\"mapped_curated_term\")\n",
        "pivot_df.columns = [f\"mapped_curated_term{col}\" for col in pivot_df.columns]\n",
        "pivot_df = pivot_df.reset_index()\n",
        "\n",
        "best_info = top5_per_term[top5_per_term[\"rank\"] == 1][[\"original_term\", \"ontology_term_id\", \"cosine_distance\"]].rename(\n",
        "    columns={\"ontology_term_id\": \"best_ontology_term_id\", \"cosine_distance\": \"best_cosine_distance\"}\n",
        ")\n",
        "\n",
        "final_df = pd.merge(pivot_df, best_info, on=\"original_term\")\n",
        "\n",
        "final_df.to_csv(\"top5_matches.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"\\nFinal result:\")\n",
        "print(final_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dba482e7",
      "metadata": {},
      "source": [
        "---\n",
        "**Work By Changchang Li**  \n",
        "March 24th, 2025"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
